{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jsonschema\n",
    "from jsonschema import validate\n",
    "\n",
    "\n",
    "#sc = pyspark.SparkContext(appName=\"ETLPipeline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
    "from py4j.protocol import Py4JJavaError\n",
    "#sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hardcoding landingpath\n",
    "landingPath='F:/interview_questions/submission/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse input json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readJSON(landingPath, basePath, fileName):\n",
    "    with open(landingPath+\"/\"+ basePath+\"//\" +fileName) as f:\n",
    "        param = json.load(f)\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read param file : \n",
    "param = readJSON(landingPath, \"inputJson\", \"param.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramSchema = readJSON(landingPath, \"schemaDef\", \"paramSchema.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validate if supplied json is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_json(json_data, schema):\n",
    "\n",
    "    try:\n",
    "        validate(instance=json_data, schema=schema)\n",
    "    except jsonschema.exceptions.ValidationError as err:\n",
    "        print(err)\n",
    "        err = \"Given JSON data is InValid\"\n",
    "        return False, err\n",
    "\n",
    "    message = \"Given JSON data is Valid\"\n",
    "    return True, message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "isUserJsonValid = validate_json(param, paramSchema)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isUserJsonValid[0] == False:\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare input dataframes and queries\n",
    "#get all variables:\n",
    "toDate = param[\"toDt\"]\n",
    "fromDate = param[\"fromDt\"]\n",
    "dateRangeFlg = param[\"dateRangeFlag\"]\n",
    "coalesce = param[\"coalesce\"]\n",
    "bucket = param[\"bucket\"]\n",
    "outputPartition = \",\".join(param[\"outputPartition\"])\n",
    "commonColumn = param[\"commonColumn\"]\n",
    "query = param[\"query\"]\n",
    "outputFileName = param[\"outputDF\"][\"fileName\"]\n",
    "outputFileType = param[\"outputDF\"][\"fileType\"]\n",
    "outputFileSep = param[\"outputDF\"][\"separator\"]\n",
    "outputFileCompression = param[\"outputDF\"][\"compressionType\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select employee.id as id, employee.name as name, department.id as deptId, employee.part_dt as part_dt, employee.hour as hour from employee inner join department on employee.dept_name=department.dept_name'"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tuning parameters can be passed by user depending on use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.memory', '8g'),\n",
       " ('spark.executor.cores', '3'),\n",
       " ('spark.cores.max', '3'),\n",
       " ('spark.driver.memory', '8g')]"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need to get spark tuning parameters as list of tuples:\n",
    "tuningParams = param[\"tuningParams\"]\n",
    "tuningParamsTuple = [(i, tuningParams[i]) for i in tuningParams]\n",
    "tuningParamsTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = pyspark.SparkConf().setAll(tuningParamsTuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create spark session for the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SamplePipeline')\\\n",
    "                    .config(conf=config) \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', 'host.docker.internal'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.cores.max', '3'),\n",
       " ('spark.driver.port', '50900'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.memory', '8g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.memory', '8g'),\n",
       " ('spark.executor.cores', '3'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1614490772162'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse input json\n",
    "import json\n",
    "from quinn import validate_schema,validate_absence_of_columns, validate_presence_of_columns\n",
    "import sys\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubtractedDate(toSubtract):\n",
    "    effDate = date.today() - timedelta(days=toSubtract)\n",
    "    return effDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28\n"
     ]
    }
   ],
   "source": [
    "prevDate = getSubtractedDate(1)\n",
    "print(prevDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read files function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read multiple type of files:\n",
    "def readInput(landingPath, fileType, fileName, delim, custom_schema,dateRangeFlg, fromDate, toDate, prevDate):\n",
    "    if dateRangeFlg == \"y\":\n",
    "        whereClause = \"part_dt >=\"+ fromDate+ \" and part_dt<=\"+ toDate\n",
    "    else:\n",
    "        whereClause = \"part_dt===\" + prevDate\n",
    "    print(whereClause)\n",
    "    if fileType == \"parquet\":\n",
    "        df  = spark.read.format('parquet').load(landingPath+\"/input/\"+fileName+\"/\").schema(custom_schema).where(whereClause).repartition(commonColumn)\n",
    "    elif fileType == \"csv\":\n",
    "        df = spark.read.format('csv').option(\"sep\",delim).option(\"header\",\"true\").load(landingPath+ \"input/\" + fileName +\"/\").where(whereClause).repartition(commonColumn)\n",
    "    elif fileType =='json':\n",
    "        df = spark.read.format('json').load(landingPath+\"/input/\"+fileName+\"/\").schema(custom_schema).repartition(commonColumn)\n",
    "    else:\n",
    "        print(\"Invalid File type\")\n",
    "    return df        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "### write files function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to write df to file:\n",
    "def writeOutput(df, landingPath, fileType, fileName, delim,outputPartitions, outputFileCompression):\n",
    "    if fileType == \"parquet\":\n",
    "        df.write.mode('overwrite').option(\"compression\",outputFileCompression).partitionBy(outputPartitions).parquet(landingPath+\"/output/\"+fileName+\"/\")\n",
    "    elif fileType == \"csv\":\n",
    "        df.write.mode('overwrite').partitionBy(outputPartitions).csv(landingPath+\"/output/\"+fileName+\"/\")\n",
    "    elif fileType =='json':\n",
    "        df.write.mode('overwrite').partitionBy(outputPartitions).json(landingPath+\"/output/\"+fileName+\"/\")\n",
    "    else:\n",
    "        raise Exception   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read schema files:\n",
    "def getSchema(landingPath,fileName):\n",
    "    rdd = sc.wholeTextFiles(landingPath+\"/schemaDef/\"+fileName+\".json\")\n",
    "    text = rdd.collect()[0][1]\n",
    "    custom_schema = StructType.fromJson(json.loads(str(text)))\n",
    "    return custom_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic schema validation using quinn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schemaValidator(custom_schema,df,tableName):\n",
    "    try:\n",
    "        validate_schema(df, custom_schema)\n",
    "    except:\n",
    "        raise Exception(tableName+\" has invalid schema\", sys.exc_info()[1])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part_dt >=20210216 and part_dt<=20210228\n",
      "part_dt >=20210216 and part_dt<=20210228\n"
     ]
    }
   ],
   "source": [
    "dfs={}\n",
    "for i in param[\"inputDF\"]:\n",
    "    custom_schema = getSchema(landingPath, i[\"table\"])\n",
    "    dfs[i[\"table\"]] = {\"dataframe\":readInput(landingPath,i[\"fileType\"],i[\"table\"],i[\"separator\"], custom_schema,\n",
    "                                             dateRangeFlg, fromDate, toDate, prevDate), \n",
    "                       \"inputSchema\":custom_schema}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'department': {'dataframe': DataFrame[id: string, dept_name: string, part_dt: int, hour: int],\n",
       "  'inputSchema': StructType(List(StructField(id,StringType,true),StructField(dept_name,StringType,true),StructField(part_dt,IntegerType,true),StructField(hour,IntegerType,true)))},\n",
       " 'employee': {'dataframe': DataFrame[id: string, name: string, dept_name: string, salary: string, part_dt: int, hour: int],\n",
       "  'inputSchema': StructType(List(StructField(id,StringType,true),StructField(name,StringType,true),StructField(salary,StringType,true),StructField(part_dt,IntegerType,true),StructField(hour,IntegerType,true),StructField(dept_name,StringType,true)))}}"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate missing or extra columns\n",
    "for tableName in dfs:\n",
    "    schemaValidator(dfs[tableName][\"inputSchema\"],dfs[tableName][\"dataframe\"],tableName)\n",
    "    dfs[tableName]['dataframe'].registerTempTable(tableName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### register as table to run user query on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read the user query and store output in a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select employee.id as id, employee.name as name, department.id as deptId, employee.part_dt as part_dt, employee.hour as hour from employee inner join department on employee.dept_name=department.dept_name'"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDF = sqlContext.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, name: string, deptId: string, part_dt: int, hour: int]"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOutput(outputDF, landingPath, outputFileType,outputFileName, outputFileSep,param[\"outputPartition\"], outputFileCompression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
